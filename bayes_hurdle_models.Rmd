---
title: 'Experiment Cutoffs: Decision-making and Bayesian Hurdle Models'
author: 'Christopher Gandrud'
output:
    tufte::tufte_html: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required packages
library(pacman)
p_load(dplyr, purrr, ggplot2, brms)

# Set ggplot2 theme
theme_set(theme_bw())
```

# Problem

Imagine that you plan to run an experiment of a new feature on an e-commerce site. You think the new feature will have a positive effect on your revenue, but don't know for sure (otherwise you wouldn't need to do the experiment).

You set up your experment using standard procedures for determining its runtime.[^runtime_simple] However, in the case that during the course of the experiment, it looks like you are losing money because the treatment seems to be perfoming worse than the control:

- how much money would be too much to lose and

- how would you know when you have lost that amount of money?

At first glance this seems like an straightforward problem. First, your boss owns the company's utility function. Ask them how much money they are willing to lose to find out the new feature's performance. Second, start the experiment and monitor the difference in revenue between the control and treatment groups. If this difference (for the same number of observations) becomes equal to or bigger than the number your boss gave you, stop the experiment.

Seems easy enough.

However, how do you know you actually lost that amount of money due to the new feature introduced as a treatment? How do you know that it wasn't also generated by chance because of **sampling error**? For example, imagine we have control and treatment groups with the following distributions for 10,000 observations each. Note, I simulated these using the same distribution,[^hurdle_dist_basic] i.e. there is actually no difference between the control and treatment groups.

```{r simfun, include=FALSE}
# Function to generate log-normal hurdle simulations

#' @param n integer number of simulations
#' @param bernoulli_prob numeric probability of jumping hurdle
#' @param lnorm_mean numeric mean of the log-normal distribution
#' @param lnorm_sd numeric standard deviation of the log-normal distribution
sim_lognormal_hurdle <- function(n = 1000,
                                 bernoulli_prob = 0.2,
                                 lnorm_mean = 3, lnorm_sd = 1) {
    # Simulate whether or not hurdle is jumped from bernoulli
    hurdle_part <- rbernoulli(n = n, p = bernoulli_prob)

    # Given hurdle has been cleared, simulate continuous value from lognormal
    is_success <- function(x) ifelse (x == 1, TRUE, FALSE)
    combined_distribution <- map_if(
        hurdle_part, is_success,
        ~ rlnorm(n = 1, meanlog = lnorm_mean, sdlog = lnorm_sd)) %>%
        unlist

    return(combined_distribution)
}
```

```{r echo=FALSE, message=FALSE, cache=TRUE, fig.cap="Observed distribution of results from an a/b test that is actually an a/a test"}
# Example simulations when from the same distribution
ab_same_dist_df <- data.frame()
for (i in c("control", "treatment")) {
    temp_df <- data.frame(variant = i,
                          revenue = sim_lognormal_hurdle(n = 10000,
                                                         bernoulli_prob = 0.6))
    ab_same_dist_df <- rbind(ab_same_dist_df, temp_df)
}

ggplot(ab_same_dist_df, aes(revenue, group = variant)) +
    facet_wrap(~variant) +
    geom_histogram(binwidth = 10) +
    ylab("Number of users\n") + xlab("\nRevenue per user")

# Find difference in revenue between a and b
nrow_df <- nrow(ab_same_dist_df)
diff_ab_same <- sum(ab_same_dist_df$revenue[1:(nrow_df/2)]) -
    sum(ab_same_dist_df$revenue[((nrow_df/2) + 1):nrow_df])

```

The difference in revenue between the control and treatment variants is `r round(diff_ab_same)`. This difference exists despite the two groups being from exactly the same distribution. There is no difference between the treatment effect and the control effect! In other words, this difference is not caused by the experiment. The experiment cost/gained us no revenue

# Solution

One solution to this problem is to take sampling error into consideration by directly **estimating how uncertain** we are about how much the experiment is costing us and compare this to how certain we want to be that the experiment is costing us a certain amount.

These are the three broad steps:

1. Specify the **range of costs** you would consider to be unacceptable to find out the effect of the treatment.

2. Us the data from your experiment to simulate our **best guess of the range of costs** that the experiment has incurred.

3. **If** these two ranges overlap, stop the test, if not keep going.

## Finding our best guess

The second step is methodologically the most tricky. Our most naive guess of the costs of our experiment is simply the difference in the total revenue between the control and treatment groups. However, as we already saw, this does not include sampling error.

We can use Bayesian methods to estimate the true cost of the experiment.[^doing_bayes] To do this we need to first understand the data we observe in the experiment and use it to estimate the range of likely costs caused by the experiment. To do this let's first think of the process that created the data, the **data generating process**:

1. A customer comes to the website and is exposed to the control or treatment. In both cases, they have the option to purchase something.

2. They decide whether or not to purchase something.

3. If they decide to purchase, they decide how much to purchase.

This data generating process suggests that the revenue data we observe from our experiments are from a **hurdle model**.[^hurdle_description] Data from a hurdle model are created by two parts. The first is a Bernoulli probability of whether or not a customer makes a purchase. The second part is some other distribution describing how much they purchase. If "how much they purchase" means number of items, then a useful distribution would be the poisson distribution for count data. In our case, we care about the Euro value of their purchase, so something like the log-normal or gamma distribution is appropriate.

The log-normal distribution is relatively easy to understand. It is simply logarithm of the normal distribution and expresses highly right skewed data, which sales per customer tend to be.

# Details with an example

Let's use this decision-making process with the data we saw above: where the control and treatment have the same effect on revenue.

First, let's set a range of unacceptable losses from the experiment: if the treatment loses more than

Second, we take our observed data and use it to simulate many different scenarios of the specific data generating processes the data are from.[^mcmc] The Bayesian jargon for this is the posterior distribution. If you are interested, the posterior parameter distributions (the posterior distributions of the parameters forming the hurdle distribution, such as the log-normal mean `b_intercept`) are shown in the margin figure. These guesses are very close to the parameters we used to generate the fake test data.[^aa_parameters]

```{r, include=FALSE, cache=TRUE, message=FALSE}
m1 <- brms::brm(revenue ~ variant, data = ab_same_dist_df,
          family = hurdle_lognormal(), cores = 4)
```

```{r, echo=FALSE, fig.margin=TRUE, fig.cap="Posterior parameters from a/b test that is actually an a/a test"}
plot(m1)
```

Now that we have many draws from the parameters' posterior distributions, we can come up with the same number of simulations of our quantity of interest: how different the revenue is between the control and treatment groups. We can do this with the following steps:

1. For each draw from the posterior find simulate the expected revenue for the control and treatment at approximately the on-going experiment's current sample size (with equal numbers for the two groups).

2. For each simulation, sum up the revenue for each group and find the difference.

3. Plot the 95% highest posterior density interval (HDI) of the distribution of simulations.

4. See if it overlaps with our stopping criteria interval. If it does, stop the test.

Let's go through these steps in detail.

Most of the work is in the first step. We need to

```{r, include=FALSE, cache=TRUE}
# Extract posterior distribution
m1_posterior <- posterior_samples(m1)

# Show column names and order
names(m1_posterior)

# Set number of observations (i.e. experiment's current sample size)
n = 10000
m1_posterior <- cbind(n_sims = n, m1_posterior)

# Control
post_control_list <- with(m1_posterior, list(n = n_sims, bernoulli_prob = hu,
                lnorm_mean = b_Intercept, lnorm_sd = sigma))

pc_sims <- pmap(post_control_list, sim_lognormal_hurdle)
pc_sims_sums <- pmap(pc_sims, sum) %>% unlist

# Treatment
post_treatment_list <- with(m1_posterior, list(n = n_sims, bernoulli_prob = hu,
                lnorm_mean = b_Intercept + b_varianttreatment, lnorm_sd = sigma))

pt_sims <- pmap(post_treatment_list, sim_lognormal_hurdle)
pt_sims_sums <- pmap(pt_sims, sum) %>% unlist
```

```{r}
hist(pc_sims_sums - pt_sims_sums)
```






[^runtime_simple]: E.g. using a sample size that gives you a power level of 80% and significance level of 5% for your anticipated treatment effect size.

[^hurdle_dist_basic]: Each simulated value $n$ is drawn from the following bernoulli-log normal hurdle probability function:
$y \sim \begin{cases} 0 & \mathrm{with\: probability}\: 0.4 \\ \ln(\mathcal{N}(20, 2.7))  & \mathrm{if}\: y > 0  \end{cases}$

[^doing_bayes]: For a discussion of why traditional confidence intervals are problematic for this problem see: <http://doingbayesiandataanalysis.blogspot.de/2013/11/optional-stopping-in-data-collection-p.html>

[^hurdle_description]: For a clear explanation of hurdle models see: <https://stats.stackexchange.com/a/81854>

[^mcmc]: I am using Markov Chain Monte Carlo (MCMC) via the Stan probablistic programming language (<http://mc-stan.org/>).

[^aa_parameters]: `b_intercept` is the log-normal mean for the control group which we had set at 3, i.e. $\exp(3) \approx 20$. `varianttreatment` is how much the treatment group's mean is different from the control group in this case they are the same, so the true value is 0. `sigma` is the standard deviation of the log-normal part, which we set to 1. `hu` is the probability of not clearing the hurdle, which we had set at 0.4.
